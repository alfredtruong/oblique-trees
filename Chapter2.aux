\relax 
\citation{nla.cat-vn178734}
\citation{nla.cat-vn2380672}
\citation{609112}
\citation{AnneB.Nattinger04011998}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Background}{{2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Classification Trees}{9}}
\newlabel{ClassificationTrees}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Natural Origins}{9}}
\newlabel{NaturalOrigins}{{2.1.1}{9}}
\newlabel{fig_taxonomy_key_biology_table}{{2.1(a)}{10}}
\newlabel{sub@fig_taxonomy_key_biology_table}{{(a)}{10}}
\newlabel{fig_taxonomy_key_biology_tree}{{2.1(b)}{10}}
\newlabel{sub@fig_taxonomy_key_biology_tree}{{(b)}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Tree-based classification is a natural way of classifying hierarchical populations and has long been used by practitioners from various fields. A Dichotomous Taxonomic Keys in particular ask a series of questions with \emph  {Yes} or \emph  {No} answers as shown in Figure\nobreakspace  {}2.1(a)\hbox {}. It is easy to follow and each path leads to a class prediction. This information can also be represented by a tree (where left branches by convention denote positive answers).}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Taxonomic Key for various species of plants.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Graphical representation of information contained in Taxonomic Key.}}}{10}}
\newlabel{fig_taxonomy_key_biology}{{2.1}{10}}
\citation{Quinlan.86}
\citation{Quinlan.88}
\citation{Quinlan.90}
\citation{cart84-2}
\citation{Sethi.Sarvarayudu.82}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Tree Growth}{11}}
\newlabel{TreeGrowth}{{2.1.2}{11}}
\newlabel{fig_typical_CART_tree_plot_tree}{{2.2(a)}{12}}
\newlabel{sub@fig_typical_CART_tree_plot_tree}{{(a)}{12}}
\newlabel{fig_typical_CART_tree_decision_boundaries}{{2.2(b)}{12}}
\newlabel{sub@fig_typical_CART_tree_decision_boundaries}{{(b)}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The above tree is grown to a training set where observations have two continuous attributes $p=2$ with observations from four classes $C\in \left \{1,\ldots  ,4\right \}$. Four dichotomous tests are applied at internal nodes to produce five leaves. The classes associated to each leaf are shown at the bottom of a tree. With $p=2$, it is possible to view the associated decision boundaries defined by the tree on the feature space $\ensuremath  {\mathbf  {X}}=\mathbb  {R}^2$. As axis-parallel splits are used to partition observations, the feature space is partitioned with rectangular blocks.}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Example of a binary splitting axis-parallel tree.}}}{12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {The associated decision boundaries defined by the tree on the feature space.}}}{12}}
\newlabel{fig_typical_CART_tree}{{2.2}{12}}
\citation{Geu02}
\citation{citeulike:161814}
\citation{cart84-2}
\citation{Ripley.96}
\citation{cart84-2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Cost-Complexity Pruning}{15}}
\newlabel{CostComplexityPruning}{{2.1.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A 3-levelled saturated tree is shown in the top-left plot with its nodes labeled. The other plots show its rooted subtrees, rows 1 to 4 showing those where neither nodes 2 and 3 are absent, row 5 showing those where only node 3 is absent, row 6 showing those where only node 2 is absent and the rest follow on the last row. One sees that the total number of rooted subtrees grows in a multiplicative manner with each additional internal node.}}{16}}
\newlabel{fig_rooted_subtrees}{{2.3}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Cost-complexity pruning allows trees that generalize well to be found. Starting with a fully grown (and thus overfit tree), a family of rooted subtrees $\left \{T_i\right \}_{i=0}^K$ is easily extracted from it to be used as candidates for the final classifier. Trees in $\left \{T_i\right \}_{i=0}^K$ are nested with increasing $i$ and each $T_i$ minimizes $R_k(T)$ over the range $k\in [k_i,k_{i+1})$. Though prediction error over the training set (in-sample error) increases with $i$, out-of-sample error needed not; the best generalizing tree is used as the final classifier. In the above example, $T_1$ say, might have the lowest out-of-sample prediction error and thus be used as the final classifier.}}{17}}
\newlabel{fig_tree_pruning}{{2.4}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Tree Interpretability}{17}}
\newlabel{TreeInterpretability}{{2.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The interpretability of a tree depends on two factors, the interpretability of each split in isolation and its cumulative effect over the entire tree. A balance should be achieved between using simple tests that are easy to understand and more complex tests that allow smaller trees to be grown.}}{18}}
\newlabel{fig_tree_interpretability}{{2.5}{18}}
\newlabel{fig_power_of_splits_oblique}{{2.6(a)}{19}}
\newlabel{sub@fig_power_of_splits_oblique}{{(a)}{19}}
\newlabel{fig_power_of_splits_axis_parallel}{{2.6(b)}{19}}
\newlabel{sub@fig_power_of_splits_axis_parallel}{{(b)}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Observations from this two-class classification problem are easily partitioned with a single oblique split as shown in Figure\nobreakspace  {}2.6(a)\hbox {}. A single axis-parallel split however cannot produce the same partitioning requiring another level of splits to achieve the same result as shown in Figure\nobreakspace  {}2.6(b)\hbox {}. Oblique splits are better able at partitioning observations so oblique trees should be smaller than axis-parallel trees.}}{19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A single oblique split is enough to partition observations in this example.}}}{19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {An axis-parallel tree requires more that one level of splits to achieve the same purity within its leaves as a single oblique split.}}}{19}}
\newlabel{fig_power_of_splits}{{2.6}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Finding Oblique Splits}{20}}
\newlabel{FindingObliqueSplits}{{2.3}{20}}
\newlabel{fig_split_dictionary_axis_parallel}{{2.7(a)}{21}}
\newlabel{sub@fig_split_dictionary_axis_parallel}{{(a)}{21}}
\newlabel{fig_split_dictionary_oblique}{{2.7(b)}{21}}
\newlabel{sub@fig_split_dictionary_oblique}{{(b)}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  A split dictionary (for a family of splits) is a set of splits of minimal size that produces all possible partitioning of observations (permitted by that family of splits) over the training set. Though split dictionaries need not be unique (small perturbations produce new split dictionaries), its size stays constant for a given dataset. Figure\nobreakspace  {}2.7(a)\hbox {} shows a possible axis-parallel split dictionary for a small dataset and is of size 6. No other partitioning of observations can result with axis-parallel splits other than those generated here. Figure\nobreakspace  {}2.7(b)\hbox {} shows a possible oblique split dictionary for the same data and is of size 15 (it is larger as oblique splits can produce different partitions of the observations).}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Example of an axis-parallel split dictionary.}}}{21}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Example of an oblique split dictionary.}}}{21}}
\newlabel{fig_split_dictionaries}{{2.7}{21}}
\citation{Cover65}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Possible Reasons for Status Quo}{23}}
\newlabel{PossibleReasonsforStatusQuo}{{2.4}{23}}
\@setckpt{Chapter2}{
\setcounter{page}{24}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{7}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
}
